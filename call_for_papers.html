
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Call for Submissions &#8212; Shift Happens (ICML 2022) 0.0.1-alpha documentation</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/custom.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Call for Reviewers" href="call_for_reviewers.html" />
    <link rel="prev" title="Shift happens: Crowdsourcing metrics and test datasets beyond ImageNet" href="index.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="index.html">
<p class="title">Shift Happens (ICML 2022)</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 current active nav-item">
 <a class="current reference internal nav-link" href="#">
  Call for Submissions
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="call_for_reviewers.html">
  Call for Reviewers
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="api.html">
  Benchmark API Docs
 </a>
</li>

    
    <li class="nav-item">
        <a class="nav-link nav-external" href="https://shift-happens-benchmark.github.io/">Home<i class="fas fa-external-link-alt"></i></a>
    </li>
    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/shift-happens-benchmark/iclr-2022" rel="noopener" target="_blank" title="Github"><span><i class="fab fa-github"></i></span>
            <label class="sr-only">Github</label></a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://pypi.org/project/shifthappens/" rel="noopener" target="_blank" title="PyPI"><span><i class="fab fa-python"></i></span>
            <label class="sr-only">PyPI</label></a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://join.slack.com/t/shifthappensicml2022/shared_invite/zt-16ewcukds-6jW6xC5DbtRvLCCkhZ~NLg" rel="noopener" target="_blank" title="Slack"><span><i class="fab fa-slack"></i></span>
            <label class="sr-only">Slack</label></a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="mailto:shifthappens@bethgelab.org" rel="noopener" target="_blank" title="Contact us!"><span><i class="fas fa-envelope"></i></span>
            <label class="sr-only">Contact us!</label></a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar">
              <div class="sidebar-start-items"><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    
  </div>
</nav>
              </div>
              <div class="sidebar-end-items">
              </div>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage mt-5 pt-1 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tl-dr">
   TL;DR
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deadlines">
   Deadlines
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#detailed-information-on-submission-types-and-topics">
   Detailed Information on Submission Types and Topics
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#submission-types">
     Submission Types
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#topics">
     Topics
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#report-instructions">
   Report Instructions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#code-and-data-instructions">
   Code and Data Instructions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluation-criteria-for-submissions">
   Evaluation Criteria for Submissions
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <section id="call-for-submissions">
<h1>Call for Submissions<a class="headerlink" href="#call-for-submissions" title="Permalink to this headline">#</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We offer up to 5 free registrations to the ICML as prizes for outstanding submissions!</p>
<p>We extended the submission deadlines:</p>
<p>The new deadline for registering abstracts is <strong>June 3</strong> (same as submission deadline).
All submissions (either an extended abstract, or a full submission in form of a technical report proof-of-concept implementation)
are due on <strong>June 3</strong>.
For authors of the <a class="reference external" href="https://neurips.cc/Conferences/2022/CallForDatasetsBenchmarks#:~:text=Abstract%20submission%20deadline%3A%20Monday%2C%20June,2022%2001%3A00%20PM%20PDT.">Neurips Datasets &amp; Benchmarks Track submissions</a>, we offer another deadline extension until <strong>June 9</strong>.</p>
<p>Submit your contribution <a class="reference external" href="https://openreview.net/group?id=ICML.cc/2022/Workshop/Shift_Happens">on OpenReview</a>.</p>
</div>
<section id="tl-dr">
<h2>TL;DR<a class="headerlink" href="#tl-dr" title="Permalink to this headline">#</a></h2>
<p>We accept the following submission types:
Full submissions and extended abstracts (<a class="reference external" href="https://drive.google.com/file/d/1bRp0Pp2ek_KbuQILyNPuOgJcUD3EuCR3/view?usp=sharing">example</a>).</p>
<p><strong>Full submissions</strong> consist of:</p>
<ul class="simple">
<li><p>a short technical report of the task, metrics and datasets</p></li>
<li><p>an implementation of the task, metrics and/or datasets, and all required external data files.</p></li>
</ul>
<p>Both components will be submitted via OpenReview, and reviewing is double-blind.</p>
<p>Please note that at submission time, a proof-of-concept implementation is sufficient.
Until the camera-ready deadline, the implementation should be comprised of a plugin to the <code class="docutils literal notranslate"><span class="pre">shifthappens</span></code> benchmark package, and the final version of the dataset needs to be provided.</p>
<p><strong>Extended abstracts</strong> consist of:</p>
<ul class="simple">
<li><p>a one or two page PDF with one figure (<a class="reference external" href="https://drive.google.com/file/d/1bRp0Pp2ek_KbuQILyNPuOgJcUD3EuCR3/view?usp=sharing">example</a>)</p></li>
</ul>
<p>In an extended abstract, the authors describe an idea for an interesting dataset or task they intend working on in the near future to get peer feedback.</p>
<p><strong>Submission</strong></p>
<p>Please submit your paper <a class="reference external" href="https://openreview.net/group?id=ICML.cc/2022/Workshop/Shift_Happens">on OpenReview</a>.
The workshop will not have any official proceedings (besides OpenReview), so it is non-archival.
Tasks that have been part of a recent submission or publication are allowed and encouraged.</p>
<p><strong>Examples</strong> of possible contributions:</p>
<ul class="simple">
<li><p>Collections of images to be evaluated w.r.t. one or more existing tasks like classification or OOD detection (e.g. ImageNet-A).
Submissions of this type can consist of (labeled) images only.</p></li>
<li><p>Re-definitions of tasks/new metrics on existing datasets
(e.g. new calibration metrics, fairness metrics, …).</p></li>
<li><p>Completely new tasks and datasets that highlight differences between ImageNet models (see below for details).</p></li>
</ul>
<p>Submissions creating links to communities interested in problems besides standard, “accuracy-focused” settings are very welcome and encouraged.</p>
<p>For general questions about preparations of submissions, clarifications around the submission score and
discussions about the <code class="docutils literal notranslate"><span class="pre">shifthappens</span></code> API, please feel free to write us as <a class="reference external" href="mailto:shifthappens&#37;&#52;&#48;bethgelab&#46;org">shifthappens<span>&#64;</span>bethgelab<span>&#46;</span>org</a>
or <a class="reference external" href="https://join.slack.com/t/shifthappensicml2022/shared_invite/zt-16ewcukds-6jW6xC5DbtRvLCCkhZ~NLg">join our slack channel</a>.</p>
</section>
<section id="deadlines">
<h2>Deadlines<a class="headerlink" href="#deadlines" title="Permalink to this headline">#</a></h2>
<p>You can find all deadlines as well as the submission page also directly <a class="reference external" href="https://openreview.net/group?id=ICML.cc/2022/Workshop/Shift_Happens">on OpenReview</a>.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://openreview.net/group?id=ICML.cc/2022/Workshop/Shift_Happens">Abstract Deadline</a>: June 3, 2022 (previously: May 27)</p></li>
<li><p><a class="reference external" href="https://openreview.net/group?id=ICML.cc/2022/Workshop/Shift_Happens">Submission Deadline for all extended abstracts and full submissions</a>: June 3, 2022 (previously: May 27)</p></li>
<li><p><a class="reference external" href="https://openreview.net/group?id=ICML.cc/2022/Workshop/Shift_Happens">Special submission Deadline</a>  for all authors of the <a class="reference external" href="https://neurips.cc/Conferences/2022/CallForDatasetsBenchmarks#:~:text=Abstract%20submission%20deadline%3A%20Monday%2C%20June,2022%2001%3A00%20PM%20PDT.">Neurips Datasets &amp; Benchmarks Track submissions</a>: June 9, 2022</p></li>
<li><p>Reviews Posted: June 13, 2022</p></li>
<li><p>Acceptance Notification: June 13, 2022</p></li>
<li><p>Camera and Dataset Ready: July 1, 2022</p></li>
<li><p>ICML 2022 Workshop dates: July 22</p></li>
</ul>
<p>Please note that it is <strong>not required</strong> to post the final dataset by the submission deadline.
It is sufficient to start working on the final dataset collection as well as the finalizing the code associated with the submission after the acceptance notification until the camera ready deadline.</p>
<p>Please also note that it is <strong>not required</strong> to post a full implementation for adding your benchmark to the <code class="docutils literal notranslate"><span class="pre">shifthappens</span></code> package by the submission deadline. You can submit any implementation along with your submission, as long at is demonstrates the applicability of your model/task to the problem setup of the workshop. We will work with all authors towards adding all accepted submissions into the final benchmark package until the camera ready deadline.</p>
</section>
<section id="detailed-information-on-submission-types-and-topics">
<h2>Detailed Information on Submission Types and Topics<a class="headerlink" href="#detailed-information-on-submission-types-and-topics" title="Permalink to this headline">#</a></h2>
<section id="submission-types">
<h3>Submission Types<a class="headerlink" href="#submission-types" title="Permalink to this headline">#</a></h3>
<p>Extended abstracts are one to two page descriptions of an idea for a task, dataset or benchmark that will not be actively collected and integrated as part of the benchmark. The submission should contain at least one summary figure and should use the standard ICML submission template.
You can find an example <a class="reference external" href="https://drive.google.com/file/d/1bRp0Pp2ek_KbuQILyNPuOgJcUD3EuCR3/view?usp=sharing">here</a>.
If you are submitting an extended abstract, no additional supplementary material is allowed.</p>
<p>Full submissions are comprised of a two page technical report, along with a code contribution submitted as supplementary material. The code should be a proof-of-concept implementation, which we will later integrate into the <code class="docutils literal notranslate"><span class="pre">shifthappens</span></code> package. If you submit a dataset, provide at least sample data along with your code as supplementary material. For large submissions, we encourage to host data externally and include instructions for downloading.</p>
</section>
<section id="topics">
<h3>Topics<a class="headerlink" href="#topics" title="Permalink to this headline">#</a></h3>
<p>Besides compatibility to ImageNet scale models, the scope of possible
benchmarks and datasets is intentionally broad:</p>
<ul class="simple">
<li><p>Submissions that <strong>provide their own evaluation criterion</strong> and discuss its value in applications are particularly encouraged. Submissions should explain why the submitted dataset and metric are well-suited to inform about the specified property.</p></li>
<li><p>It is also encouraged to submit <strong>datasets</strong> that can be evaluated with one of the following <strong>standard criteria</strong>:
- Robustness to domain shifts (classification accuracy)
- Out-of-distribution detection (AUROC, FPR, AUPR)</p></li>
<li><p>Any other form of dataset and task that can be evaluated on a pre-trained (standard or non-standard training) ImageNet model.</p></li>
</ul>
<p>There are <em>no constraints</em> on the possible metrics, as long as they are based on the features, class scores,
class uncertainties and in-distribution scores of such a model (also see our <a class="reference external" href="https://shift-happens-benchmark.github.io/icml-2022/">reference implementation</a> for examples).</p>
<p>In all cases, it is possible to re-submit <strong>existing and potentially published</strong> benchmarks, datasets, and evaluation tasks to
consolidate them in one benchmark suite and/or if they are known only to a particular community. Examples include small datasets that test an
interesting distribution shift, such as shifts occurring due to applications in the real world, and
insightful benchmarks that you might have included in a publication highlighting the advantages or problems
of certain models.</p>
<p>Submissions are allowed to contain <strong>multiple related datasets</strong>, e.g.,
a dataset like ImageNet-C could have been submitted as a collection of
15 evaluation datasets, corresponding to the different corruptions
ImageNet-C is comprised of.</p>
<p>Correspondingly, tasks do not need to output one single number. For example, a
submission might include multiple (related) OOD datasets and demand that an
ideal model be not fooled by any of them. It might of course makes sense for a
<strong>multi-score</strong> benchmark to <em>also</em> calculate an average performance.</p>
</section>
</section>
<section id="report-instructions">
<h2>Report Instructions<a class="headerlink" href="#report-instructions" title="Permalink to this headline">#</a></h2>
<p>The short report should</p>
<ul class="simple">
<li><p>motivate why the submitted task is interesting,</p></li>
<li><p>describe how the data was collected, as well as give an overview over the data,</p></li>
<li><p>state how the data can be accessed,</p></li>
<li><p>specify if there are special requirements on the models to be evaluated,</p></li>
<li><p>detail the evaluation procedure and outline how the evaluation outputs can be interpreted,</p></li>
<li><p>provide a short analysis how the task is challenging for some existing models
(including the relevant provided ones),</p></li>
<li><p>and establish context within related works.</p></li>
</ul>
<p>The report should be limited to 2-4 pages without references.
If it includes an Appendix, it should be reserved for additional
sample images and technical details.</p>
<p>For the submission, the report should be formatted according to the <a class="reference external" href="https://icml.cc/Conferences/2022/StyleAuthorInstructions">ICML style instructions</a>, by using the
provided <a class="reference external" href="https://media.icml.cc/Conferences/ICML2022/Styles/icml2022.zip">LaTeX files</a>.</p>
</section>
<section id="code-and-data-instructions">
<h2>Code and Data Instructions<a class="headerlink" href="#code-and-data-instructions" title="Permalink to this headline">#</a></h2>
<p><strong>Submissions</strong> should demonstrate the full capability of your dataset/task/benchmark, but do not need to contain a final implementation, a full dataset, etc. yet. Make sure to submit a code sample and (parts of) the dataset as supplementary material to your paper submission, directly on OpenReview. Please make sure that from the submitted code it becomes clear how a model would be evaluated. While we invite you to directly build on top of the provided <a class="reference external" href="https://shift-happens-benchmark.github.io/icml-2022/">reference implementation</a>, this is not a requirement at submission time (for example, it it acceptable to provide code for a reference run of a ResNet50 model, or whatever is suitable for your task, even outside the <code class="docutils literal notranslate"><span class="pre">shifthappens</span></code> package). If you have questions about implementation, please do not hesitate to reach out via email or our slack channel. We will continue to assist authors of accepted submissions to make their submission ready for integration to the <code class="docutils literal notranslate"><span class="pre">shifthappens</span></code> package.</p>
<p>Until the camera-ready deadline, all submissions need to be updated to include a link to the dataset (hosted on a suitable platform),
as well as code (building on top of the provided <a class="reference external" href="https://shift-happens-benchmark.github.io/icml-2022/">reference implementation</a>) for
running the evaluation process. Datasets can be hosted on <a class="reference external" href="https://zenodo.org/">zenodo</a>,
<a class="reference external" href="https://www.google.com/drive/">google drive</a> (by only providing an anonymous google drive ID), or other platforms.</p>
<p>The data/images need to be usable for research purposes. Their license should
be stated in the report.</p>
<p>Please refer to the <a class="reference external" href="https://shift-happens-benchmark.github.io/api.html">API Docs</a> for further information on how to implement benchmarks and datasets directly in the <code class="docutils literal notranslate"><span class="pre">shifthappens</span></code> package (not required, but encouraged at submission time).</p>
</section>
<section id="evaluation-criteria-for-submissions">
<h2>Evaluation Criteria for Submissions<a class="headerlink" href="#evaluation-criteria-for-submissions" title="Permalink to this headline">#</a></h2>
<p>Submissions will be judged according to the following criteria:</p>
<ol class="arabic simple">
<li><p><strong>Correctness:</strong> For labeled datasets, the labels should make sense to a
human reviewer. For OOD datasets, no in-distribution objects can be
visible on the images. During the review of large datasets, random
samples and the worst mistakes of some models will be checked. The
correctness will mainly be reviewed based on the submitted dataset
and the technical report.</p></li>
<li><p><strong>Novelty</strong>: Datasets which allow for a more insightful evaluation beyond
the standard test accuracy of ImageNet are encouraged.
This can include well-motivated new criteria, new datasets with emphasized
practical relevance, as well as tasks that demonstrate theoretically
predicted weaknesses of certain popular models.</p></li>
<li><p><strong>Difficulty for current models</strong>: If the task can easily be solved by
humans but some models fail moderately or spectacularly, it is an
interesting addition to the benchmark.
This will formally be benchmarked by evaluating a set of standard models
(including robustified, task-specific ones) on the
provided task.</p></li>
</ol>
</section>
</section>


              </div>
              
              
          </main>
          

      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021 - 2022, Julian Bitterwolf, Evgenia Rusak, Steffen Schneider, Roland S. Zimmermann and contributors. Released under an Apache 2.0 License.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.5.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>